{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48392d40",
   "metadata": {},
   "source": [
    "**LIBRARYS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52605e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\JT2ju\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: '#API': Expected package name at the start of dependency specifier\n",
      "    #API\n",
      "    ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\JT2ju\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: '#Dataframes': Expected package name at the start of dependency specifier\n",
      "    #Dataframes\n",
      "    ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\JT2ju\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: '#Arrays': Expected package name at the start of dependency specifier\n",
      "    #Arrays\n",
      "    ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\JT2ju\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: '#ML': Expected package name at the start of dependency specifier\n",
      "    #ML\n",
      "    ^\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\JT2ju\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: '#imbalance': Expected package name at the start of dependency specifier\n",
      "    #imbalance\n",
      "    ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-api-python-client #API handling\n",
    "%pip install pandas #Dataframes\n",
    "%pip install numpy  #Arrays\n",
    "%pip install scikit-learn #ML tools\n",
    "%pip install imblearn #imbalance \n",
    "\n",
    "from googleapiclient.discovery import build #to call API\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.pipeline import Pipeline #chains our models tgt\n",
    "from xgboost import XGBClassifier #gradient boosted model\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #convert to TF-IDF features (higher importance to low occurance)\n",
    "from sklearn.ensemble import RandomForestClassifier #builds multiple decision trees and combines for better results\n",
    "from imblearn.over_sampling import SMOTE #Synthetic Minority Over-sampling Technique\n",
    "from imblearn.under_sampling import RandomUnderSampler #randomly removes from majority\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d41d7d",
   "metadata": {},
   "source": [
    "*DATASET* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28d586cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('allcomments_labled.csv') #Change File location if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b830c05",
   "metadata": {},
   "source": [
    "*STAGE 1 MODEL*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97bc30a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.79      0.76       823\n",
      "           1       0.47      0.43      0.45       213\n",
      "           2       0.66      0.49      0.56       215\n",
      "           3       0.54      0.39      0.45        36\n",
      "           4       0.69      0.71      0.70       219\n",
      "\n",
      "    accuracy                           0.68      1506\n",
      "   macro avg       0.62      0.56      0.58      1506\n",
      "weighted avg       0.67      0.68      0.67      1506\n",
      "\n",
      "Accuracy: 0.6759628154050464\n"
     ]
    }
   ],
   "source": [
    "X = dataset['text']\n",
    "y = dataset['sentiment']\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=30000, max_df=0.7, min_df=5, ngram_range=(1, 3)) #ignores less than 5, more than 70%, and also considers trigrams\n",
    "X_tfidf = vectorizer.fit_transform(X) #transforms into TF-IDF\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority', random_state=42) #undersample majority\n",
    "oversample = SMOTE(sampling_strategy='not majority', random_state=42) #oversample minority\n",
    "\n",
    "pipeline = Pipeline(steps=[('o', oversample), ('u', undersample)]) #chains SMOTE and RUS tgt\n",
    "\n",
    "X_train_balanced, y_train_balanced = pipeline.fit_resample(X_train, y_train) #balance\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_balanced), y=y_train_balanced)\n",
    "class_weights_dict = dict(zip(np.unique(y_train_balanced), class_weights)) #dictionary where class is mapped to weight ->adjust learning for each class\n",
    "\n",
    "xgb_model = XGBClassifier(scale_pos_weight=class_weights_dict, random_state=42) #scale pos weight adjusts to class weights\n",
    "xgb_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Evaluation\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4861cc5e",
   "metadata": {},
   "source": [
    "*API FUNCTION*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e76f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"AIzaSyC5qnX0CLK3CYerum4WpsqopEtnlzQHN-I\" #May need to change\n",
    "youtube = build('youtube', 'v3', developerKey=api_key) #setup API\n",
    "\n",
    "\n",
    "def fetch_all_comments(video_id, max_results=100): #get all comments for video\n",
    "    comments = [] #empty array\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            response = youtube.commentThreads().list(\n",
    "                part='snippet', #details on comment\n",
    "                videoId=video_id,\n",
    "                pageToken=next_page_token, #next comment\n",
    "                maxResults=max_results,  #get up to max_results per request\n",
    "                textFormat='plainText'\n",
    "            ).execute()\n",
    "        except Exception as e: #error handling\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break\n",
    "\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comments.append({\n",
    "                'author': comment['authorDisplayName'],\n",
    "                'text': comment['textDisplay'],\n",
    "                'like_count': comment['likeCount'],\n",
    "                'published_at': comment['publishedAt'] #details appended to comments and attach to empty array\n",
    "            })\n",
    "            #print(f\"Added comment from {comment['authorDisplayName']}\")\n",
    "\n",
    "        next_page_token = response.get('nextPageToken') #updates token/node\n",
    "        if not next_page_token:\n",
    "            break #ends if last\n",
    "\n",
    "    return comments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5ba158",
   "metadata": {},
   "source": [
    "*HELPER FUNCTION*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "754dc123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_youtube_video_title(video_id, api_key):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key) \n",
    "    request = youtube.videos().list(part=\"snippet\", id=video_id)\n",
    "    response = request.execute()\n",
    "    \n",
    "    if response[\"items\"]:\n",
    "        title = response[\"items\"][0][\"snippet\"][\"title\"]\n",
    "        #print(\"Video Title:\", title)\n",
    "        return title  # return title\n",
    "    else:\n",
    "        print(\"Video not found.\")\n",
    "        return None\n",
    "    \n",
    "def interpret_sentiment(predicted_sentiment):\n",
    "    sentiment_mapping = {\n",
    "        1: \"pleased\",\n",
    "        2: \"funny\",\n",
    "        3: \"fear\",\n",
    "        4: \"sad\"\n",
    "    }\n",
    "    \n",
    "    return sentiment_mapping.get(predicted_sentiment, \"Unknown sentiment\") #matches each number to sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58acd0e",
   "metadata": {},
   "source": [
    "**FINAL CODE**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "078dcfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment for video 'Mr Bean Goes Shopping... | Mr Bean Live Action | Funny Clips | Mr Bean': funny\n"
     ]
    }
   ],
   "source": [
    "input_file_path = r'trainingimproved.csv' #change file location if necessary\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "X = df[['Count_0', 'Count_1', 'Count_2', 'Count_3', 'Count_4']]  \n",
    "y = df['Actual Sentiment'] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42, class_weight='balanced') #random forest model\n",
    "\n",
    "rf_model.fit(X_train, y_train) #training data\n",
    "\n",
    "y_pred = rf_model.predict(X_test) #tests on testing data (from train test split)\n",
    "\n",
    "# Evaluate the model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Process video ID and use RF model\n",
    "def predict_final_sentiment(video_id):\n",
    "    comments = fetch_all_comments(video_id)\n",
    "\n",
    "    comments_df = pd.DataFrame(comments)\n",
    "    sorted_comments = comments_df.sort_values(by='like_count', ascending=False) #sorts comments by like value\n",
    "    valid_predictions = []\n",
    "\n",
    "    for index, row in sorted_comments.iterrows():\n",
    "        if len(valid_predictions) == 30:  #only take top 30\n",
    "            break\n",
    "        \n",
    "        text = row['text'] #extract text\n",
    "        text_tfidf = vectorizer.transform([text]) #TF-IDF\n",
    "        prediction = xgb_model.predict(text_tfidf)[0] #predict each comment\n",
    "        if 0 <= prediction <= 4:\n",
    "            valid_predictions.append(prediction) \n",
    "\n",
    "    prediction_counts = Counter(valid_predictions)\n",
    "    counts = [prediction_counts.get(i, 0) for i in range(0, 5)] #count each and add to array\n",
    "    predicted_sentiment = rf_model.predict([counts])[0]  #Final Predicted based on array\n",
    "    \n",
    "    return predicted_sentiment\n",
    "\n",
    "# CHANGE THIS\n",
    "video_id = 'nIHyr_fp_yI'  \n",
    "\n",
    "title = get_youtube_video_title(video_id, api_key)\n",
    "if title:\n",
    "    predicted_sentiment = predict_final_sentiment(video_id)\n",
    "    sentiment_label = interpret_sentiment(predicted_sentiment)\n",
    "    print(f\"Predicted sentiment for video '{title}': {sentiment_label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
